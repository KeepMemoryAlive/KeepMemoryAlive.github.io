---
layout: post
title: "Linux 网络收包过程"
date: 2020-05-22 13:23:49 +0800
comments: true
categories: Linux
tags: [linux, 收包, 网卡]
keywords: 网卡, 中断, 
description: linux 网卡收包 中断分析
---

## 简要

网络数据 从网卡到socket之间 系统是怎么处理的？涉及哪些系统消耗？在分析服务器性能时，了解这些原理往往能给我们很大帮助。

<!-- more -->

### 名词简介
- DMA : Direct Memory Access 直接内存访问，是外设硬件的特性，不需要通过CPU操作。
- NAPI ：New Api 网卡驱动程序的一个框架，可以提高网络性能。
- NIC : Network Interface Controller 即网卡
- RX ：Receive X
- TX : Transmit X 
- IRQ : Interrupt Request ---> a hardware signal sent to the processor that temporarily stops a running program and allows a special program (有的说法:硬中断、硬件中断)
- Softirq : 中断处理的后半部分,只是它处理的是相对不那么紧急，显得"soft"一些的部分.
- ksoftirqd :  kernel softirq daemon 内核处理softirq的守护进程
- RPS ： Receive Packet Steering 内核的一个特性 用来把softirq分给指定的cpu处理

## 过程总览

1. 数据包从网络到达NIC 
2. 数据包通过DMA被拷贝到一块特殊内存中
3. NIC产生一个IRQ 通知系统 数据包已经在内存中了
4. 系统处理IRQ，调用对应中断处理程序即NAPI,去处理内存中的数据
5. 处理完后，NAPI最后会产生一个softirq
6. 最后ksoftirqd程序处理softirq把数据包丢给内核协议栈

整个过程中有几点需要注意:
- 第2步中，不同的NIC可能拥有的队列数不一致，即常说的“**网卡是否多队列**”，很明显队列越多 网卡性能越好.
- **每个队列对应一个IRQ中断号**, 比如八队列网卡，在/proc/interrupt里搜索eth相关即可得到8个中断号
- Cpu从第4步处理IRQ开始参与，根据中断号n, 在/proc/irq/n/smp_affinity中可以查看 该中断由哪些cpu处理
- 第6步处理softirq的cpu, 如果不开启RPS的话, 会由上一步的同一个cpu接着处理
- **softirq并不是指“软件中断”，它其实也是处理硬件中断的一部分，只是延后处理，所以叫soft.**

### softirq机制
中断处理“下半部”机制

中断服务程序一般都是在中断请求关闭(屏蔽)的条件下执行的，以避免嵌套而使中断控制复杂化。但是，中断是一个随机事件，它随时会到来，如果关中断的时间太长，CPU就不能及时响应其他的中断请求，从而造成中断的丢失，因此，Linux内核的目标就是尽可能快的处理完中断请求，尽其所能把更多的处理向后推迟。

所以IRQ处理程序一定很快很简单，处理完后就解除屏蔽，并产生一个softirq，通知系统处理后续未完事宜，这就是softirq的由来。
因此，内核把中断处理其实分为了两部分：上半部（top-half）和下半部（bottom-half），上半部IRQ（就是中断服务程序）内核立即执行，而下半部softirq（就是一些内核函数）留着稍后处理.

**top命令中的%si 就是softirq所占的cpu使用率**

**/proc/softirqs 中可以看到各个cpu处理的softirq统计，NET_RX即网络收包中断**

### RPS
Receive Packet Steering 可以认为是网卡多队列的一个软件实现；

对网卡多队列而言，因为有多个队列，所以有多个中断IRQ，接着softirq就会被多个cpu处理。

而对网卡单队列而言，只有一个队列，一个中断号IRQ，由上文得知，softirq也会被同一个cpu处理

设置rps后，即处理softirq的cpu就不一定和处理IRQ的cpu一致，可以均匀分配给多个cpu

在网卡单队列或者双队列，但是cpu8核甚至更多核的场景，rps是很有作用的

设置RPS非常简单：/sys/class/net/eth*/queues/rx-*/rps_cpus 文件写入cpu mask, 比如cpu0+cpu1 即设置为3
不用其他操作直接生效

## 性能分析

整个过程中那些地方对服务器性能有影响呢？

1. 网卡队列数肯定越多 网卡性能越好
2. 网卡队列产生的IRQ 可以通过/proc/irq/n/smp_affinity绑定指定cpu来执行, 也是设置cpu mask，即时生效
也可以设置为多个cpu，，这样IRQ会被分布到多个cpu执行，但是在实际应用中，一般把单个中断绑定单一固定cpu，
已提高cpu缓存命中率，提高性能。

为什么要将中断绑定到固定CPU？

    In order to achieve the best performance, it is recommended that all the interruptions
    generated by a device queue is handled by the same CPU, instead of IRQ balancing.
    Although it is not expected, round robin IRQ distribution is not good for performance 
    because when the interruption go to another fresh CPU, the new CPU
    probably will not have the interrupt handler function in the cache,
    and a long time will be required to get the properly interrupt handler 
    from the main memory and run it. On the other hand, if the same CPU handles
    the same IRQ almost all the time, the IRQ handler function will unlikely 
    leave the CPU cache, boosting the kernel performance。

3. RPS设置可以灵活运用，避免单个cpu的si占用率过高，并且当si过高时，该cpu上的用户态程序会受严重影响，
花费很多cpu用在上下文切换；**所以实际应用中，应该避免单个cpu si过高 或者 应用程序避开si过高的cpu**。

4. **多队列网卡 队列够多的情况下，比如8队列；通过分开绑定proc/irq/n/smp_affinity到单个cpu, 后续的softirq自然也分布到不同的cpu上，就无需再设置RPS**