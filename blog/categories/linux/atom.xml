<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Linux | coding]]></title>
  <link href="http://blog.youngxiao.com/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://blog.youngxiao.com/"/>
  <updated>2020-05-26T21:26:47+08:00</updated>
  <id>http://blog.youngxiao.com/</id>
  <author>
    <name><![CDATA[young]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Linux 网络收包过程]]></title>
    <link href="http://blog.youngxiao.com/blog/newwork-card/"/>
    <updated>2020-05-22T13:23:49+08:00</updated>
    <id>http://blog.youngxiao.com/blog/newwork-card</id>
    <content type="html"><![CDATA[<h2>简要</h2>

<p>网络数据 从网卡到socket之间 系统是怎么处理的？涉及哪些系统消耗？在分析服务器性能时，了解这些原理往往能给我们很大帮助。</p>

<!-- more -->


<h2>名词简介</h2>

<ul>
<li>DMA : Direct Memory Access 直接内存访问，是外设硬件的特性，不需要通过CPU操作。</li>
<li>NAPI ：New Api 网卡驱动程序的一个框架，可以提高网络性能。</li>
<li>NIC : Network Interface Controller 即网卡</li>
<li>RX ：Receive X</li>
<li>TX : Transmit X</li>
<li>IRQ : Interrupt Request &mdash;> a hardware signal sent to the processor that temporarily stops a running program and allows a special program (有的说法:硬中断、硬件中断)</li>
<li>Softirq : 中断处理的后半部分,只是它处理的是相对不那么紧急，显得"soft"一些的部分.</li>
<li>ksoftirqd :  kernel softirq daemon 内核处理softirq的守护进程</li>
<li>RPS ： Receive Packet Steering 内核的一个特性 用来把softirq分给指定的cpu处理</li>
</ul>


<h2>过程总览</h2>

<ol>
<li>数据包从网络到达NIC</li>
<li>数据包通过DMA被拷贝到一块特殊内存中</li>
<li>NIC产生一个IRQ 通知系统 数据包已经在内存中了</li>
<li>系统处理IRQ，调用对应中断处理程序即NAPI,去处理内存中的数据</li>
<li>处理完后，NAPI最后会产生一个softirq</li>
<li>最后ksoftirqd程序处理softirq把数据包丢给内核协议栈</li>
</ol>


<p>整个过程中有几点需要注意:</p>

<ul>
<li>第2步中，不同的NIC可能拥有的队列数不一致，即常说的“<font color=red>网卡是否多队列</font>”，很明显队列越多 网卡性能越好.</li>
<li><font color=red>每个队列对应一个IRQ中断号</font>, 比如八队列网卡，在/proc/interrupt里搜索eth相关即可得到8个中断号</li>
<li>Cpu从第4步处理IRQ开始参与，根据中断号n, 在/proc/irq/n/smp_affinity中可以查看 该中断由哪些cpu处理</li>
<li>第6步处理softirq的cpu, 如果不开启RPS的话, 会由上一步的同一个cpu接着处理</li>
<li><font color=red>softirq并不是指“软件中断”，它其实也是处理硬件中断的一部分，只是延后处理，所以叫soft.</font></li>
</ul>


<h2>softirq机制</h2>

<p>中断处理“下半部”机制</p>

<p>中断服务程序一般都是在中断请求关闭(屏蔽)的条件下执行的，以避免嵌套而使中断控制复杂化。但是，中断是一个随机事件，它随时会到来，如果关中断的时间太长，CPU就不能及时响应其他的中断请求，从而造成中断的丢失，因此，Linux内核的目标就是尽可能快的处理完中断请求，尽其所能把更多的处理向后推迟。</p>

<p>所以IRQ处理程序一定很快很简单，处理完后就解除屏蔽，并产生一个softirq，通知系统处理后续未完事宜，这就是softirq的由来。
因此，内核把中断处理其实分为了两部分：上半部（top-half）和下半部（bottom-half），上半部IRQ（就是中断服务程序）内核立即执行，而下半部softirq（就是一些内核函数）留着稍后处理.</p>

<p><font color=red>top命令中的%si 就是softirq所占的cpu使用率</font></p>

<p><font color=red>/proc/softirqs 中可以看到各个cpu处理的softirq统计，NET_RX即网络收包中断</font></p>

<h2>RPS</h2>

<p>Receive Packet Steering 可以认为是网卡多队列的一个软件实现；</p>

<p>对网卡多队列而言，因为有多个队列，所以有多个中断IRQ，接着softirq就会被多个cpu处理。</p>

<p>而对网卡单队列而言，只有一个队列，一个中断号IRQ，由上文得知，softirq也会被同一个cpu处理</p>

<p>设置rps后，即处理softirq的cpu就不一定和处理IRQ的cpu一致，可以均匀分配给多个cpu</p>

<p>在网卡单队列或者双队列，但是cpu8核甚至更多核的场景，rps是很有作用的</p>

<p>设置RPS非常简单：/sys/class/net/eth<em>/queues/rx-</em>/rps_cpus 文件写入cpu mask, 比如cpu0+cpu1 即设置为3
不用其他操作直接生效</p>

<h2>性能分析</h2>

<p>整个过程中那些地方对服务器性能有影响呢？</p>

<ol>
<li>网卡队列数肯定越多 网卡性能越好</li>
<li>网卡队列产生的IRQ 可以通过/proc/irq/n/smp_affinity绑定指定cpu来执行, 也是设置cpu mask，即时生效
也可以设置为多个cpu，这样IRQ会被分布到多个cpu执行，但是在实际应用中，一般把单个中断绑定单一固定cpu，
已提高cpu缓存命中率，提高性能。</li>
</ol>


<p>为什么要将IRQ中断绑定到固定CPU？</p>

<pre><code>In order to achieve the best performance, it is recommended that all the interruptions
generated by a device queue is handled by the same CPU, instead of IRQ balancing.
Although it is not expected, round robin IRQ distribution is not good for performance 
because when the interruption go to another fresh CPU, the new CPU
probably will not have the interrupt handler function in the cache,
and a long time will be required to get the properly interrupt handler 
from the main memory and run it. On the other hand, if the same CPU handles
the same IRQ almost all the time, the IRQ handler function will unlikely 
leave the CPU cache, boosting the kernel performance。
</code></pre>

<ol>
<li><p>RPS设置可以灵活运用，避免单个cpu的si占用率过高，并且当si过高时，该cpu上的用户态程序会受严重影响，
花费很多cpu用在上下文切换；<font color=red>所以实际应用中，应该避免单个cpu si过高 或者 应用程序避开si过高的cpu</font>。</p></li>
<li><p><font color=red>多队列网卡 队列够多的情况下，比如8队列；通过分开绑定proc/irq/n/smp_affinity到不同的cpu, 后续的softirq自然也分布到不同的cpu上，就无需再设置RPS, si均分各个cpu，且没有cpu缓存命中率问题.</font></p></li>
<li><p>最后总结下，在网卡收包处理中，先是IRQ中断(硬)处理, 再是softirq处理; 网卡单队列或者双队列，可能存在si集中问题，八队列或者更大队列数最优，即分散了中断处理，又不会影响cpu缓存。</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux Ip命令详解]]></title>
    <link href="http://blog.youngxiao.com/blog/net/"/>
    <updated>2020-03-21T17:23:49+08:00</updated>
    <id>http://blog.youngxiao.com/blog/net</id>
    <content type="html"><![CDATA[<h2>背景</h2>

<p>在业务机器的部署中,有的业务监听多个vip(virtual ip) + rsport(real server port)的情形,初看之下这种监听不合常理:
- 为啥监听虚拟ip+真实端口？
- 为啥不能直接监听0.0.0.0,而要采用复杂的配置多个ip端口?</p>

<p>由这个问题衍生出许多linux的基础知识,本文一起记录下!</p>

<!-- more -->


<h2>ip 命令</h2>

<p>ip 命令非常强大,拥有非常多的功能,主要有ip link, ip addr, ip tunnel, ip route, ip rule等等&hellip;</p>

<h3>ip link: 网络设备配置命令</h3>

<p>可以通过<code>man ip-link</code>来查看官方手册.</p>

<ul>
<li><code>ip link show</code>显示本机所有的网络设备</li>
<li><code>ip link add ...</code>添加虚拟设备, <strong>每个网络设备不一定都是物理设备,可能是虚拟设备,比如bridge\tunnel等</strong></li>
<li><code>ip link set tunl0 up|down</code>启动\关闭设备</li>
</ul>


<p><img src="https://raw.githubusercontent.com/KeepMemoryAlive/KeepMemoryAlive.github.io/master/images/ip_link_show.png" alt="" /></p>

<ul>
<li>UP 表示处于启动的状态;BROADCAST 表示可以发送广播包;MULTICAST 可以发送多播;LOWER_UP表示网线插着.</li>
<li>link/ether 这一行表示MAC地址，是设备的物理地址; brd:指广播地址</li>
</ul>


<h3>ip addr：协议地址管理命令</h3>

<p>可以通过<code>man ip-address</code>来查看官方手册.</p>

<pre><code>The address is a protocol (IP or IPv6) address attached to a network device. 
Each device must have at least one address to use the corresponding protocol. 
It is possible to have several different addresses attached to one device.  
These addresses are not discriminated, 
so that the term alias is not quite appropriate for them and
we do not use it in this document.
</code></pre>

<ul>
<li><strong>ip地址对应一个网络设备,设备不一定是网卡,可以是tunnel等</strong></li>
<li><strong>单个网络设备至少有一个ip地址,可以有多个</strong>,ip地址之间可以配置成对等.</li>
<li><code>ip addr</code> 直接显示所有ip配置
<img src="https://raw.githubusercontent.com/KeepMemoryAlive/KeepMemoryAlive.github.io/master/images/ip_addr_show.png" alt="" /></li>
<li><p>scope(范围):Host、link、global</p>

<pre><code>  Host: An address has a host scope when it is used only to communicate within the host itself. 
  Outside the host this address is not known and can not be used.
  An Example is the loopback address, 127.0.0.1

  Link: An address has a link scope when it is meaningful 
  and can be used only within a LAN. An example is a subnet's broadcast address.

  Global: An address has global scope when it can be used anywhere. T
  his is the default scope for most addresses. (...)
</code></pre></li>
<li><p><code>ip addr show dev|scope</code> 根据dev设备 或者 scope范围显示ip配置</p></li>
<li><code>ip addr add 192.168.1.1/24 dev tunl0 label tunl0:0</code> 给Eth1添加一个ip地址,并打上标签eth1:0.
如果给已有ip的设备添加ip地址，后面的地址会被设为secondary
<img src="https://raw.githubusercontent.com/KeepMemoryAlive/KeepMemoryAlive.github.io/master/images/ip_addr_add.png" alt="" />
  如果添加的ip地址是不同网段,或者掩码/32, 则不会有secondary标志。</li>
</ul>


<h3>ip tunnel：隧道配置命令</h3>

<p>TUN 是 Linux 内核实现的一种<strong>虚拟网络设备</strong></p>

<p>IP隧道(IP tunneling)是<strong>将一个IP报文封装在另一个IP报文的技术</strong>，这可以使得目标为一个IP地址的数据报文能被封装和转发到另一个IP地址。IP隧道技术亦称为IP封装技术(IP encapsulation)。IP隧道主要用于虚拟私有网络(Virtual Private Network)，在其中隧道都是静态建立的，隧道一端有一个IP地址，另一端也有唯一的IP地址.
可以通过<code>man ip-tunnel</code>来查看官方手册</p>

<ul>
<li><p>ip tunnel支持多种mode，ipv4支持 ipip, sit, isatap and gre; ipv6支持ip6ip6, ipip6 and any.</p></li>
<li><p>ipip即IPv4 in IPv4，在 IPv4 报文的基础上再封装一个IPv4报文,用tcpdump会发现,有两个ip header.
<img src="https://raw.githubusercontent.com/KeepMemoryAlive/KeepMemoryAlive.github.io/master/images/ipip.png" alt="" /></p></li>
</ul>


<p>可以理解为<strong>当数据包通过tunl发送出去时,会自动添加一层ip header，当通过tunl接收数据时,会自动剥掉一层ip header</strong>.</p>

<ul>
<li>ip tunnel 列出所有的隧道.</li>
<li><p>ip tunnel add tunl0 mode ipip remote 10.10.12.1 local 10.20.12.1 dev eth1</p>

<p>添加一个隧道网络设备,注意这只是添加设备,并不涉及ip地址, 设备和ip地址一定要分开理解,这时候通过ifconfig可以看到并没有增加ip地址.</p>

<p><strong>‘dev eth1’ 可有可无,如果有：代表该虚拟tunnel绑定该dev,所有该tunnel的包由该dev路由经过</strong>.</p>

<p>这时候可以通过ip addr给该tunnel添加一个或者多个ip地址  这个ip地址即我们常说的"虚拟ip".</p></li>
</ul>


<h3>ip route</h3>

<p>路由表(route table)为每个数据包寻找一条最佳的传输路径，并将该数据有效地传送到目的站点。</p>

<p>/etc/iproute2/rt_tables中记录了路由表名和id的对应关系,系统默认有3个表 为local\main\default.</p>

<p><strong>默认我们操作的route针对main表</strong>, local由内核自己维护,一般不需要关心.可以自定义id从1－252的路由表</p>

<p>可以通过<code>man ip-route</code>查看官方手册</p>

<p><code>ip route</code> 或者 <code>route -n</code> 命令会列出main表所有路由,只是格式不一致,&lsquo;-n&rsquo; 指把'default'这种数字化表示.
<img src="https://raw.githubusercontent.com/KeepMemoryAlive/KeepMemoryAlive.github.io/master/images/route.png" alt="" /></p>

<p><strong>ip route结果可以看出,有的路由没有via字段,即不需要通过网关.</strong>
route -n命令结果更加直观</p>

<ul>
<li>Destination : 目标网络或目标主机。Destination 为 default（0.0.0.0）时，表示这个是默认网关，所有数据都发到这个网关（这里是 10.139.128.1）</li>
<li>Gateway : 需要通过的网关地址，0.0.0.0 表示当前记录对应的 Destination 跟本机在同一个网段，通信时不需要经过网关</li>
<li>Genmask : Destination 字段的网络掩码，Destination 是主机时需要设置为 255.255.255.255，是默认路由时会设置为 0.0.0.0</li>
<li>Flags : U-路由是活动的 H-目标是个主机 G-需要经过网关 !-拒绝路由</li>
<li><p>Iface : 对应的输出接口</p></li>
<li><p>添加或者删除路由
<code>ip route add|delete 10.12.20.1 via 9.134.32.1 dev eth1 table 1</code> 如果不指定table,则默认加到main表里</p></li>
<li>查看路由
<code>ip route list table ***</code> 查看指定table的路由项</li>
</ul>


<h3>ip rule</h3>

<p>route table里通过数据包的dest地址来选择路由地址, 路由策略rule提供了能够根据报文大小、应用或IP源地址等属性来选择route table.</p>

<p>简单来说：<strong>linux系统有多张route table，而路由策略rule会根据一些条件，将路由请求转向不同的route table</strong></p>

<p>一条路由策略rule主要包含三个信息，即rule的<strong>优先级，条件，路由表</strong>。
其中rule的优先级数字越小表示优先级越高，然后是满足什么条件下由指定的路由表来进行路由。
在linux系统启动时，内核会为路由策略数据库配置三条缺省的规则，
即rule 0(local)，rule 32766(main)， rule 32767(default)（数字是rule的优先级).
系统会按照<strong>rule的优先级顺序依次匹配</strong>.</p>

<ul>
<li><code>ip rule</code> 会列出所有的rule</li>
<li><code>ip rule add from|to 10.12.20.2 table 1  pref 1000</code><br/>
所有src|dest为10.12.20.2的数据包查找route table 1 优先级为1000,如果pref数字不填,则从32765递减插入</li>
</ul>


<p><strong>多个策略rule可以指向同一张路由表(route table)。某些路由表可以没有策略指向它,那么这个表就是没有用的,
但它在系统中仍然存在,直到route table中的内容全部删除.</strong></p>

<h3>举个例子</h3>

<p>在一个云计算网络中，很多虚拟子机构成一个局域网，每台子机只有一个网卡&mdash;局域网ip(内网ip), 标记为C.</p>

<p>外部接入时从接入层机器接入,接入层机器有两个网卡,一个配置外网ip 标记为A, 一个配置内网ip 标记为B.</p>

<p>同时我们在子机中添加一个tunl,remote:接入层机ip B, local:C  并且给tunl绑定一个ip也为A, 这个A对子机而言就是虚拟IP.</p>

<p>一个数据包srcip:cltip  dstip: A  收包过程：</p>

<ul>
<li>首先数据包到达接入层外网网卡.</li>
<li>接入层机器通过配置查询知道,数据包需要发给哪台子机.</li>
<li>给数据包加一层ipip，添加的srcip:B  dstip:C, 然后通过内网网卡发往子机.</li>
<li>子机内网网卡收到数据包，通过ipip头里的srcip对比，找到对应的tunl.</li>
<li>数据包被转到tunl，剥掉ipip头, 得到原始数据包，抛给内核,内核再抛给应用程序.</li>
<li>应用程序得到的数据包里：srcip:clintip,  dstip:接入层的外网ip.</li>
<li>所以应用程序要么监听0.0.0.0, 要么监听外网ip 才能捕获到数据.</li>
</ul>


<p><strong>再讨论回包过程前，需要先了解下，数据包里srcip的机制</strong>。
通过观察api的参数，会发现无论tcp,还是udp, 都没有参数指定srcip.</p>

<ul>
<li>如果是tcp,因为是长链接强管理,五元组是确定的 (srcip,srcport,dstip,dstport,tcp),所以srcip一定是A.</li>
<li>如果是udp,udp程序在收包前会bind ip+port，<strong>注意一个socket只能bind一个ip+port</strong>,
如果bind了具体的ip(非0.0.0.0),则通过该socket回包,srcip则一定是对应bind的ip.</li>
<li><strong>如果是udp,且bind 0.0.0.0,如果本机有多个ip地址, 则回包时,srcip的选择由内核路由结果得出，会选择和路由结果相近(同网段)的ip地址</strong></li>
<li><strong>udp,如果想绑定多个ip地址,又不想绑定0.0.0.0,则只有通过生成多个socket来实现.</strong></li>
</ul>


<p>应用程序处理后回包 srcip:A  dstip:cltip   回包过程：</p>

<ul>
<li>为了顺利回包,会配置from A的路由规则，路由到tunl</li>
<li>则数据包会到达tunl，添加一层ipip，srcip:C dstip:B</li>
<li>然后数据包通过内网网卡发送出去.</li>
<li>接入机器内网网卡收到数据包,解开ipip头，由外网网卡发送出去.</li>
</ul>


<h2>总结</h2>

<p>所以文本开头的两个问题，都是为了完成收发包，才必须那么做。</p>

<p>实际生产环境中，tunl不止一个，接入外网ip也不止一个，则子机中有多个虚拟ip，</p>

<p>对于udp而言，如果监听0.0.0.0，那么回包的srcip，根据路由结果而定，那么回包路径则可能不通。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux 多核cpu负载分析]]></title>
    <link href="http://blog.youngxiao.com/blog/cpuinfo/"/>
    <updated>2020-03-09T19:23:49+08:00</updated>
    <id>http://blog.youngxiao.com/blog/cpuinfo</id>
    <content type="html"><![CDATA[<h2>背景:</h2>

<p>一台linux服务器，多核cpu，部署多个进程</p>

<ul>
<li>这台服务器的cpu压力是怎样的？</li>
<li>单个进程的cpu使用率是怎么算的？</li>
<li>多个核的使用率是均衡的吗？</li>
<li>哪些原因会影响各个核使用率？</li>
</ul>


<!-- more -->


<h2>cpu使用率：</h2>

<p>cpu使用率指 进程在一段时间内占用cpu的时间 / 单个cpu总共的时间.</p>

<p><strong>单进程单线程同时只能在一个cpu上执行，所以它最多100%。</strong></p>

<p>就算它在多个核上调度执行，比如2核 1s时间内进程在cpu0 执行0.5s,在cpu1执行0.5s,总使用率还是100%.</p>

<p><strong>多线程可以在多个核上同时执行，则使用率可以达到N*100%, N指几个线程</strong></p>

<p>从cpu使用率的定义得知：使用率越高代表进程越繁忙。</p>

<h2>现代服务器的cpu构架：</h2>

<p><strong>非统一内存访问架构</strong>（英语：Non-uniform memory access，简称NUMA）是一种为<strong>多处理器</strong>的电脑设计的内存架构，内存访问时间取决于内存相对于处理器的位置。在NUMA下，处理器访问它自己的本地内存的速度比非本地内存（内存位于另一个处理器，或者是处理器之间共享的内存）快一些。</p>

<p>最初的架构是这样的：
<img src="https://raw.githubusercontent.com/KeepMemoryAlive/KeepMemoryAlive.github.io/master/images/no_numa.png" alt="" /></p>

<p>随着从“拼频率”到“拼核心数”的转变，越来越多的核心被尽可能地塞进了同一块芯片上，各个核心对于内存带宽的争抢访问成为了瓶颈.</p>

<p> NUMA在这种情况下发展而来：通过把 多个CPU和部分RAM 当做一个 node，整体分为多个node, CPU 会优先访问距离近的 RAM,cpu跨node访问消耗更大。</p>

<p> <img src="https://raw.githubusercontent.com/KeepMemoryAlive/KeepMemoryAlive.github.io/master/images/numa_node.png" alt="" /></p>

<p>注意图中这里的一块cpu 是一个物理芯片，一般有多个core,而不是单个cpu core。</p>

<p><strong>NUMA相关的几个概念有node、socket、core和thread。一个node里可以有多个Socket, Socket是一个物理上的概念，指的是主板上的cpu插槽(即一个物理芯片), 一个socket拥有多个物理core,core即一个独立的硬件执行单元。Thread就是超线程的概念，是一个逻辑cpu，共享core上的执行单元，一个物理core上可以有2个Thread，即2个逻辑cpu.</strong></p>

<p>所以现在的cpu层级是这样的：
<img src="https://raw.githubusercontent.com/KeepMemoryAlive/KeepMemoryAlive.github.io/master/images/numa.png" alt="" /></p>

<p>从图中可以看出：这里的coreP1 coreP2&hellip;是逻辑cpu 即超线程cpu, L1和L2缓存都是每个物理CPU一个, L1缓存有分为L1i和L1d，分别用来存储指令和数据，L2缓存是不区分指令和数据的。<strong>L3缓存是同socket的多个物理核心共用一个。不同socket的cpu是不共用任何cache的</strong>。</p>

<p>常用命令: numactl &ndash;hardware, numactl &ndash;show, numastat等</p>

<h2>超线程(Hyper-Threading)：</h2>

<p>通过此技术，英特尔实现在一个实体CPU中，提供两个逻辑线程(即逻辑cpu),英特尔于2016年发布的Core i7-6950X便是将10核心的处理器，加上超线程技术，使之成为20个逻辑线程的产品。</p>

<p>超线程技术就是利用特殊的硬件指令，把两个逻辑内核模拟成两个物理芯片，让单个处理器都能使用线程级并行计算． 具体讲，就是通过CPU的寄存器构成了两个逻辑处理器，来共享处理器的物理执行单元，并同步进行加、乘、负载等操作，当两个线程都同时需要某一个资源时，其中一个要暂时停止，并让出资源，直到这些资源闲置后才能继续，因此<strong>超线程的性能并不等于两颗CPU的性能</strong>。</p>

<p>/proc/cpuinfo中的各项数值含义:
<img src="https://raw.githubusercontent.com/KeepMemoryAlive/KeepMemoryAlive.github.io/master/images/cpuinfo.png" alt="" /></p>

<pre><code>processor: 即逻辑cpu
physical id: 即上文所说的socket，即物理芯片(注意不是物理单核)
core id：即物理单核core
siblings：即同一个socket下的逻辑cpu个数

所以如果两个processor的physical_id和core_id都相同，则这两个processor为同一个物理core里的超线程cpu，即启用了超线程。
</code></pre>

<p>通过<code>lscpu -p</code> <code>lscpu -e</code>可以清楚看到各个cpu参数
<img src="https://raw.githubusercontent.com/KeepMemoryAlive/KeepMemoryAlive.github.io/master/images/lscpu.png" alt="" /></p>

<h2>调度器：</h2>

<p>在现代这样复杂的系统中，调度器要解决的一个首要问题就是如何发挥这么多 CPU 的性能，使得负载均衡。不能存在某些 CPU一直很忙，进程在排队等待运行，而某些 CPU却是处于空闲状态。</p>

<ul>
<li><p>单核调度：</p>

<p>  对运行在单核上的线程进行调度，每个核都有自己的运行队列，最新的算法称为CFS，它的基本原理是：给每个在调度队列中的线程一个vruntime的变量，记录这个线程运行了多久，每次调度，都调度vruntime最小的线程。</p></li>
<li><p>多核调度:</p>

<p>  单核调度是多核调度的基础，每个核有自己的运行队列，还需要考虑各个核之间的任务的平衡，当一个核特别忙，另一个特别闲，则会进行核之间的Load Balance。但是Load Balance 是有代价的：</p>

<ol>
<li>同一个物理核的两个超线程，它们之前迁移代价最小。</li>
<li>跨物理核迁移，会导致L1 L2 cache失效。</li>
<li>跨node迁移，不仅导致L1 L2 L3 cache失效，还会导致cpu和内存之间的访问距离变大，直接影响线程的运行效率。</li>
<li>cpu power(功耗)问题，一个物理 CPU 中的两个超线程CPU 各执行一个进程，显然比两个物理 CPU 中的  超线程CPU 各执行一个进程节省功耗。</li>
</ol>
</li>
</ul>


<p>为了解决上面的问题，Linux2.6 中引入基于 Scheduling Domains(调度域) 的解决方案。</p>

<p>从上文得知，系统的层级为Numa Node&ndash;>Physical CPU(物理芯片) &ndash;> Multi core(单个芯片多个core)&mdash;>Hyper threading(单个core包含2个HT),
<strong>所以在同一个系统中,各个processor之间并不是完全对等的,它们所在的层级分支,决定了它们之间的关系！</strong></p>

<p>每个 Scheduling Domain 其实就是根据层级划分具有相同属性的一组 cpu 的集合,不同级之间通过指针链接在一起，从而形成一种的树状的关系.
<img src="https://raw.githubusercontent.com/KeepMemoryAlive/KeepMemoryAlive.github.io/master/images/domain.jpg" alt="" /></p>

<p>负载平衡就是针对 Scheduling domain 的。从叶节点往上遍历。直到所有的 domain 中的负载都是平衡的。当然<strong>对不同的 domain 会有不同的策略识别是否负载不平衡，以及不同的调度策略</strong>。通过这样的方式，从而很好的发挥众多 cpu 的效率. 层级越高,调度的频率会越低.</p>

<p>目录/proc/sys/kernel/sched_domain 下有所有的最底层级别的cpu目录, 目录里有对应的domain0\domain1\domain3等。。。</p>

<p>根据系统的实际层级,目录里可能只有一个domain,或者有多个domain,一般情况下：</p>

<pre><code>domain0/name--&gt;"SIBLING"  
domain1/name--&gt;"MC" (multi core)  
domain2/name--&gt;"CPU" 
domain3/name--&gt;"NUMA"
</code></pre>

<p>domain目录里有各种参数配置文件，具体含义需要参见内核算法,比较好理解的两个参数文件：min_interval(最小的 load balance 间隔)
max_interval(最大的 load balance 间隔),不同的domain里的参数并不相同。</p>

<p>对于开发者而言，调整domain的算法的情况很少，我们只需要记住:<strong>在多node多物理cpu的系统下，各个processor之间并不是对等的，所以它们的cpu使用率可能会有一定程度的不均等</strong>,</p>

<p>根据实际情况调整系统的层级部署，减少高层级的调度。单node单物理cpu 肯定比 多node多物理cpu 产生的调度消耗小。</p>
]]></content>
  </entry>
  
</feed>
